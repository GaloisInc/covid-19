#! /usr/bin/env python3

import data_pipelines.util

import click
import json
import os
import pandas as pd
import shutil

TARGET = 'website/public/d'

@click.command()
def main():
    """Run this script to re-build the website/public/d directory, which
    contains up-to-date information from `data_pipelines`.
    """
    _clean_target()
    _county_list()
    _county_data()


def _clean_target():
    """Reset target directory.
    """
    if os.path.lexists(TARGET):
        shutil.rmtree(TARGET)
    os.makedirs(TARGET)
    with open(os.path.join(TARGET, 'README.md'), 'w') as f:
        f.write('This folder generated by website-data.py -- any changes '
                'will automatically be overwritten.')


def _county_data():
    """Build per-county data.
    """
    import data_pipelines.county_nytimes_covid_stats as covid_stats
    import data_pipelines.state_covidtracking_com_covid_testing as testing
    import data_pipelines.county_usda_census as usda_census
    import data_pipelines.county_descarteslabs_mobility as mobility

    cs = covid_stats.get()
    st = testing.get()
    census = usda_census.get()
    m = mobility.get()

    # Join county information
    county_date_data = [
            cs[['county', 'date', 'state', 'cases', 'deaths']],
            m[['county', 'date', 'm50', 'm50_index', 'samples']].rename(
                columns=dict(m50='mobility_m50', m50_index='mobility_m50_index',
                    samples='mobility_samples')),
    ]
    county_data = [
            census['education'][['county']],
            census['population'][['county', 'POP_ESTIMATE_latest']].rename(
                columns=dict(POP_ESTIMATE_latest='population')),
            census['unemployment'][['county']],
    ]
    state_data = [
            st[['state', 'date', 'positive', 'negative', 'pending']].rename(
                columns=dict(positive='state_test_positive',
                    negative='state_test_negative',
                    pending='state_test_pending')),
    ]

    def rollup(data, idx):
        df = None
        for d in data:
            for i in idx:
                d = d[~pd.isna(d[i])]

            if df is None:
                df = d
            else:
                df = df.merge(d, on=idx, how='left')
        return df

    county_df = rollup(county_date_data, ['county', 'date'])
    county_df = rollup([county_df] + county_data, ['county'])
    county_df = rollup([county_df] + state_data, ['state', 'date'])

    # Data layout is {county: {key: [values in date ascending order]}}
    bucket = None
    bucket_seen = set()
    bucket_data = None
    def save_bucket():
        nonlocal bucket, bucket_data
        if bucket is None:
            return

        with open(os.path.join(TARGET, 'county_date_' + bucket + '.json'), 'w') as f:
            json.dump(bucket_data, f)
        bucket = None
        bucket_data = None

    for row_idx, row in county_df.sort_values(['county', 'date']).iterrows():
        b = row['county'][:4]
        if b != bucket:
            save_bucket()
            if b in bucket_seen:
                raise ValueError(f'Duplicate bucket {b}')
            bucket = b
            bucket_seen.add(b)
            bucket_data = {}

        county = bucket_data.setdefault(row['county'], {})
        data = dict(row)
        data.pop('county')
        for k, v in data.items():
            all_v = county.setdefault(k, [])
            # Pandas' NaN values do not translate well to browser-compatible
            # JSON.
            v = v if not pd.isna(v) else None
            all_v.append(v)

    save_bucket()


def _county_list():
    """Builds a list of all available counties.  Used for search.
    """
    fips = data_pipelines.util.fips

    entries = {}

    for state_fips, counties in fips._counties.items():
        for _, county_fips in counties.items():
            f = state_fips + county_fips
            if f.startswith('00') or county_fips == '000':
                # Skip full US / full county data at the moment, until
                # it's better supported.
                continue
            entries[f] = data_pipelines.util.resolve_county_name_full(f)

    entries = [[k, v] for k, v in entries.items()]
    entries.sort(key=lambda x: x[0].lower())
    with open(os.path.join(TARGET, 'counties.tsv'), 'w') as f:
        f.write('\n'.join(['\t'.join(v) for v in entries]))


if __name__ == '__main__':
    main()

